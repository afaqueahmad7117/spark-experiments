{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Imports & Configuration </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25578ece-41dd-49fa-807e-7b5798d875d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h2> Simulating Uniform Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2134bcf-2fd7-41c9-9fc1-32328c196482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|0    |\n",
      "|1    |\n",
      "|2    |\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_uniform = spark.createDataFrame([i for i in range(1000000)], IntegerType())\n",
    "df_uniform.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a6b2e9-3543-4f5d-82ee-3f7e33139318",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|partition| count|\n",
      "+---------+------+\n",
      "|        0|249856|\n",
      "|        1|249856|\n",
      "|        2|249856|\n",
      "|        3|250432|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_uniform\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Skewed Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|id |\n",
      "+---+\n",
      "|0  |\n",
      "|1  |\n",
      "|2  |\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = spark.range(0, 1000000).repartition(1)\n",
    "df1 = spark.range(0, 10).repartition(1)\n",
    "df2 = spark.range(0, 10).repartition(1)\n",
    "df_skew = df0.union(df1).union(df2)\n",
    "df_skew.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a33df63-459d-4bb3-88d2-5189334be1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=========================================>             (150 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|partition|  count|\n",
      "+---------+-------+\n",
      "|        0|1000000|\n",
      "|        1|     10|\n",
      "|        2|     10|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Skews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_file = \"../../data/data_skew/transactions.parquet\"\n",
    "customer_file = \"../../data/data_skew/customers.parquet\"\n",
    "\n",
    "df_transactions = spark.read.parquet(transactions_file)\n",
    "df_customers = spark.read.parquet(customer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- start_date: string (nullable = true)\n",
      " |-- end_date: string (nullable = true)\n",
      " |-- txn_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- expense_type: string (nullable = true)\n",
      " |-- amt: string (nullable = true)\n",
      "\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "|cust_id   |start_date|end_date  |txn_id         |date      |year|month|day|expense_type |amt   |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "|C2AU14903J|2013-04-01|2019-05-01|TRGA5GWBO0CY0F3|2014-02-17|2014|2    |17 |Motor/Travel |58.25 |\n",
      "|C2AU14903J|2013-04-01|2019-05-01|THCM719A8W1I5MT|2017-05-13|2017|5    |13 |Entertainment|27.03 |\n",
      "|C2AU14903J|2013-04-01|2019-05-01|TZGMSM7SEKCLIAK|2014-09-12|2014|9    |12 |Entertainment|61.12 |\n",
      "|C2AU14903J|2013-04-01|2019-05-01|TFUNCYRAH5E9L0C|2018-01-01|2018|1    |1  |Entertainment|159.77|\n",
      "|C2AU14903J|2013-04-01|2019-05-01|T6FQ56TQ6EXFDPA|2014-11-19|2014|11   |19 |Entertainment|39.7  |\n",
      "+----------+----------+----------+---------------+----------+----+-----+---+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transactions.printSchema()\n",
    "df_transactions.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- distinct_txns: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      "\n",
      "+----------+-------------+-------------+---+------+----------+-----+\n",
      "|cust_id   |distinct_txns|name         |age|gender|birthday  |zip  |\n",
      "+----------+-------------+-------------+---+------+----------+-----+\n",
      "|C000BK8N2S|6949         |Aaron Abbott |34 |Female|7/13/1991 |97823|\n",
      "|C005K7U9RE|6540         |Aaron Austin |37 |Female|12/16/2004|30332|\n",
      "|C006CT8BVO|6356         |Aaron Barnes |29 |Female|3/11/1977 |23451|\n",
      "|C007YEYTX9|7445         |Aaron Barrett|31 |Male  |7/9/1998  |46613|\n",
      "|C00B971T1J|7532         |Aaron Becker |54 |Male  |11/24/1979|40284|\n",
      "+----------+-------------+-------------+---+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customers.printSchema()\n",
    "df_customers.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|cust_id   |ct      |\n",
      "+----------+--------+\n",
      "|C0YDPQWPBJ|43551962|\n",
      "|CXD6UZEGKS|6999    |\n",
      "|CP2GC38KPG|6999    |\n",
      "|CQZK7HS7HL|6999    |\n",
      "|C1MZ9FNHAN|6999    |\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_transactions\n",
    "    .groupBy(\"cust_id\")\n",
    "    .agg(F.countDistinct(\"txn_id\").alias(\"ct\"))\n",
    "    .orderBy(F.desc(\"ct\"))\n",
    "    .show(5, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_txn_details = (\n",
    "    df_transactions.join(\n",
    "        df_customers,\n",
    "        on=\"cust_id\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "127125002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_txn_details.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-skew-explanation",
   "notebookOrigID": 4018749166498458,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
